{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os \n",
    "import pandas as pd\n",
    "from dataClass import DataTable\n",
    "from main import list_files_in_folder\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "load_dotenv()\n",
    "\n",
    "f = open('nogit/HardTablesR1_Valid_CEA_ER.json') \n",
    "\n",
    "data = json.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(s1, s2):\n",
    "    \"\"\"\n",
    "    Calculate the Levenshtein distance between two strings.\n",
    "\n",
    "    Parameters:\n",
    "    s1 (str): The first string.\n",
    "    s2 (str): The second string.\n",
    "\n",
    "    Returns:\n",
    "    int: The Levenshtein distance between the two strings.\n",
    "    \"\"\"\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "\n",
    "    # len(s1) >= len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    return previous_row[-1]\n",
    "\n",
    "def jaccard_distance(s1, s2, n=1):\n",
    "    \"\"\"\n",
    "    Calculate the Jaccard distance between two strings based on n-grams.\n",
    "\n",
    "    Parameters:\n",
    "    s1 (str): The first string.\n",
    "    s2 (str): The second string.\n",
    "    n (int): The length of n-grams to consider. Default is 1 (character-wise comparison).\n",
    "\n",
    "    Returns:\n",
    "    float: The Jaccard distance between the two strings.\n",
    "    \"\"\"\n",
    "    # Generate n-grams for both strings\n",
    "    def ngrams(string, n):\n",
    "        return {string[i:i+n] for i in range(len(string) - n + 1)}\n",
    "    \n",
    "    set1 = ngrams(s1, n)\n",
    "    set2 = ngrams(s2, n)\n",
    "    \n",
    "    # Calculate the intersection and union of the two sets\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    \n",
    "    # Handle the case where both sets are empty\n",
    "    if not union:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate the Jaccard index\n",
    "    jaccard_index = len(intersection) / len(union)\n",
    "    \n",
    "    # Return the Jaccard distance\n",
    "    return 1 - jaccard_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_str(df):\n",
    "    column_names = df.columns.tolist()\n",
    "    table_str = \"col: \"\n",
    "    table_str += \"| \" + \" | \".join(column_names) + \" | \"\n",
    "    for index, row in df.iterrows():\n",
    "        row_str = \" | \" + \" | \".join(str(row[col]) for col in column_names) + \" | \"\n",
    "        table_str += f\"[SEP] col {index + 1}: {row_str}\"\n",
    "    return table_str\n",
    "\n",
    "def candidates_as_str(candidates):\n",
    "    \n",
    "    list_of_candidates = \"\"\n",
    "    for c in candidates:\n",
    "        if c['description'] == '':\n",
    "            c['description'] = 'None'\n",
    "        list_of_candidates += f\"<[ID] {c['id']} [NAME] {c['name']} [DESC] {c['description']} [TYPE] {c['types'][0]['name']}>, \"\n",
    "    \n",
    "    return list_of_candidates[:-2]\n",
    "\n",
    "def build_prompt(table_str, column_name, cell_content, candidates, t_desc):\n",
    "    TASK = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION = \"### Instruction: This is an entity linking task. The goal for this task is to link the selected entity mention in the table cells to the entity in the knowledge base. You will be given a list of referent entities, with each one composed of an entity id, name, its description and its type. Please choose the correct one from the referent entity candidates. Note that the Wikipedia page, Wikipedia section and table caption (if any) provide important information for choosing the correct referent entity.\"\n",
    "    INPUT = f\"### Input: [TLE] {t_desc} [TAB] {table_str}\"\n",
    "    QUESTION = f\"### Question: The selected entity mention in the table cell is: {cell_content}. The column name for ’{cell_content}’ is {column_name}. \"\n",
    "    CANDIDATES = f\"The referent entity candidates are: {candidates}\"\n",
    "    tablellama_prompt = (\n",
    "        f\"{TASK}\\n\\n\"\n",
    "        f\"{INSTRUCTION}\\n\\n\"\n",
    "        f\"{INPUT}\\n\\n\"\n",
    "        f\"{QUESTION}\"\n",
    "        f\"{CANDIDATES}. \\nIf there are no candidates that matched the cell content the response is <NIL>. What is the correct referent entity for the entity mention ’{cell_content}’ ?\\n\\n\"  \n",
    "        \"### Response: \"\n",
    "    )\n",
    "    return tablellama_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_path = 'data/HardTablesR1/DataSets/HardTablesR1/Valid/gt/cea_gt.csv'\n",
    "tables_path = 'data/HardTablesR1/DataSets/HardTablesR1/Valid/tables'\n",
    "\n",
    "gt = pd.read_csv(gt_path, header=None)\n",
    "tables = list_files_in_folder(tables_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "target_names = list(data.keys())\n",
    "print(len(target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "535"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = gt[gt[0].isin(target_names)]\n",
    "len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "535it [00:00, 45540.30it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "correct = 0\n",
    "for i, (name, r, c, l)  in tqdm(filtered_df.iterrows()):\n",
    "    # print(i, name, c, r, l)\n",
    "    \n",
    "    target_id = l.split('/')[-1]\n",
    "    # print(f\"Ground Truth: {target_id}\")\n",
    "    \n",
    "    ids_list = [d['id'] for d in data[name][str((r, c))]['retrieved_list']]\n",
    "    # print(f\"Target list of ids: {ids_list}\\n\")\n",
    "    # print(f\"Is the ground truth in the target list?\\n{target_id in ids_list}\\n\\n\")\n",
    "    if target_id in ids_list:\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Accuracy: 0.56\n"
     ]
    }
   ],
   "source": [
    "print(f\"Retrieval Accuracy: {correct/len(filtered_df):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = data['ZRWO683W']['(1, 0)']['cell']\n",
    "s2 = data['ZRWO683W']['(1, 0)']['retrieved_list'][0]['name'].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def score(s1, s2):\n",
    "    w_lev = 10\n",
    "    w_jac = 8\n",
    "    lev_sim = levenshtein_distance(s1, s2)\n",
    "    jac_sim = jaccard_distance(s1, s2)\n",
    "    \n",
    "    return w_lev*lev_sim + w_jac*jac_sim\n",
    "\n",
    "score(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_distance(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ZRWO683W']['(1, 0)']['retrieved_list'][0]\n",
    "prova = [(el['id'], ) for el in data['ZRWO683W']['(1, 0)']['retrieved_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import generate_tableDesc_prompt, generate_CEA_prompt_with_t_desc\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "mistral_api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "model_22 = \"open-mixtral-8x22b\"\n",
    "model_7 = \"open-mixtral-8x7b\"\n",
    "llm_22 = ChatMistralAI(model=model_22, temperature=0, api_key=mistral_api_key)\n",
    "llm_7 = ChatMistralAI(model=model_7, temperature=0, api_key=mistral_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "table_name = None \n",
    "y_true, y_pred = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Table_name: None\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "235it [06:27,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in parsing the ouput\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "535it [14:35,  1.64s/it]\n"
     ]
    }
   ],
   "source": [
    "index = []\n",
    "export = {}\n",
    "print(f\"\\n\\nTable_name: {table_name}\\n\")\n",
    "for i, (name, r, c, l)  in tqdm(filtered_df.iterrows()):\n",
    "    # print(i, name, c, r, l)\n",
    "    \n",
    "    if name != table_name:\n",
    "        table_name = name\n",
    "        table = DataTable(f\"data/HardTablesR1/DataSets/HardTablesR1/Valid/tables/{name}.csv\")\n",
    "        export[name] = {}\n",
    "        # Generate table description\n",
    "        table.generate_t_description(llm_7)\n",
    "        table_as_str = get_table_str(table.data)\n",
    "\n",
    "    # print(table.data)\n",
    "    \n",
    "    target_id = l.split('/')[-1]\n",
    "    # print(f\"\\nGround Truth: {target_id}\\n\")\n",
    "    \n",
    "    ids_list = [d['id'] for d in data[name][str((r, c))]['retrieved_list']]\n",
    "    \n",
    "    if target_id in ids_list:\n",
    "\n",
    "        # Perform CEA:\n",
    "        cell_content = data[name][str((r, c))]['cell']\n",
    "        prompt = generate_CEA_prompt_with_t_desc(table.data, cell_content, candidates_as_str(data[name][str((r, c))]['retrieved_list']), table.t_desc)\n",
    "        #print(f\"\\nPrompt:\\n{prompt}\\n\\n\")\n",
    "        out = llm_7.invoke(prompt)\n",
    "        time.sleep(2)\n",
    "        # print(out.content)\n",
    "        y_true.append(target_id)\n",
    "        y_pred.append(out.content)\n",
    "        index.append(i)\n",
    "        export[name][str((r, c))] = {\n",
    "            'cell': cell_content,\n",
    "            'table_desc': table.t_desc,\n",
    "            'cea_prompt': prompt,\n",
    "            'cea_model': llm_7.model,\n",
    "            'model_out': out.content\n",
    "        }\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = [el.strip('[[[').strip(']]]') for el in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.00%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for true, pred in zip(y_true, y_preds):\n",
    "    if true == pred:\n",
    "        correct += 1\n",
    "print(f\"Accuracy: {correct/len(y_true)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nogit/results_8x7_Cea_sub80.json', 'w') as f:\n",
    "    json.dump(export, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giofratti/miniforge3/envs/STI/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "A new version of the following files was downloaded from https://huggingface.co/apple/OpenELM-450M-Instruct:\n",
      "- configuration_openelm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/apple/OpenELM-450M-Instruct:\n",
      "- modeling_openelm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"apple/OpenELM-450M-Instruct\", trust_remote_code=True) \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Encode the prompt into tokens\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[43mprompt\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m55\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Generate text\u001b[39;00m\n\u001b[1;32m      5\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m      6\u001b[0m     inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m      7\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m      8\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, \n\u001b[1;32m      9\u001b[0m     pad_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prompt' is not defined"
     ]
    }
   ],
   "source": [
    "# Encode the prompt into tokens\n",
    "inputs = tokenizer(prompt[:-55], return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "outputs = model.generate(\n",
    "    inputs['input_ids'], \n",
    "    attention_mask=inputs['attention_mask'], \n",
    "    max_length=2000, \n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode the generated tokens back into text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.3 ('STI')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2268df63e3314f5c0fa267e7a7d58ca881c28135e668c4afe664cdf6d7ddd66d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
